{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddad216-f39b-450e-a6d4-cf3c9d081d96",
   "metadata": {},
   "source": [
    "Detailed Plan:\n",
    "1.Import Required Libraries:\n",
    "2.Import pandas to read the CSV and MinMaxScaler from scikit-learn to perform normalization.\n",
    "\n",
    "Load the CSV File:\n",
    "    Use pandas.read_csv to load the menu data into a DataFrame.\n",
    "    Apply Normalization:\n",
    "        Use MinMaxScaler on the numeric columns (Calories, Carbs, Proteins, Fats, Fiber) to scale them to the range [0, 1].\n",
    "    Combine the Data:\n",
    "        Combine the scaled numeric data with the non-numeric data (like Meal_Type and Food_Item).\n",
    "    Save or Use the Normalized Data:\n",
    "        You can save the normalized DataFrame back into a CSV or use it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294c8087-462d-49ce-8c4b-255e943de722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.1.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading numpy-2.1.1-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.1 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f4d2ec-7ac0-4211-b56c-c00de98b9b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (2.1.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00184dce-f180-4485-92a5-2207c6116496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/Temporary/Downloads/menu_data.csv')\n",
    "\n",
    "# Define numeric columns\n",
    "numeric_columns = ['Carbs (g)', 'Protein (g)', 'Fat (g)', 'Fiber (g)']\n",
    "\n",
    "# Function to clean and convert columns\n",
    "def clean_numeric_column(col):\n",
    "    return pd.to_numeric(col.str.extract(r'(\\d+\\.?\\d*)')[0], errors='coerce')\n",
    "\n",
    "# Clean each numeric column\n",
    "for col in numeric_columns:\n",
    "    df[col] = clean_numeric_column(df[col])\n",
    "\n",
    "# Handle NaN values by filling with 0\n",
    "df[numeric_columns].fillna(0, inplace=True)\n",
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler() # the following function  is using  the standard function more details :  https://www.perplexity.ai/search/keyerror-carbs-proteins-fats-f-LR13aQRWSRm4OnoVGWBpGw#5\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Output cleaned DataFrame for verification\n",
    "print(df[numeric_columns])\n",
    "\n",
    "# Save the normalized data back into a CSV file if needed\n",
    "df.to_csv('normalized_menu_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fc1f73-ac29-485b-a827-747f7abe7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Carbs (g)  Protein (g)  Fat (g)  Fiber (g)\n",
      "0      0.3000     0.106667    0.016       0.10\n",
      "1      0.5000     0.133333    0.400       0.40\n",
      "2      0.6250     0.333333    0.120       1.00\n",
      "3      0.1225     0.086667    0.176       0.34\n",
      "4      0.7250     0.366667    0.328       0.20\n",
      "5      0.6250     0.266667    0.040       0.40\n",
      "6      0.0500     0.800000    0.560       0.00\n",
      "7      0.0000     0.000000    0.160       0.00\n",
      "8      0.3250     0.000000    0.000       0.00\n",
      "9      0.0000     0.000000    0.000       0.00\n",
      "10     0.1250     0.066667    0.080       0.00\n",
      "11     1.0000     1.000000    0.400       0.40\n",
      "12     0.1250     0.200000    0.080       0.00\n",
      "13     0.2500     1.000000    1.000       0.20\n",
      "14     0.7500     0.333333    0.240       0.60\n",
      "15     0.1500     0.066667    0.000       0.40\n",
      "16     0.8250     0.200000    0.000       0.12\n",
      "17     0.4750     0.400000    0.160       1.00\n",
      "18     0.5000     0.133333    0.320       0.60\n",
      "19     0.0750     0.200000    0.080       0.00\n",
      "20     0.3750     0.133333    0.040       0.40\n",
      "21     0.3000     0.533333    0.160       0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/wl7ddbqs0_d8c_g077wn429h0000gp/T/ipykernel_22339/1884913457.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[numeric_columns].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler  # Change here\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/Temporary/Downloads/menu_data.csv')\n",
    "\n",
    "# Define numeric columns\n",
    "numeric_columns = ['Carbs (g)', 'Protein (g)', 'Fat (g)', 'Fiber (g)']\n",
    "\n",
    "# Function to clean and convert columns\n",
    "def clean_numeric_column(col):\n",
    "    return pd.to_numeric(col.str.extract(r'(\\d+\\.?\\d*)')[0], errors='coerce')\n",
    "\n",
    "# Clean each numeric column\n",
    "for col in numeric_columns:\n",
    "    df[col] = clean_numeric_column(df[col])\n",
    "\n",
    "# Handle NaN values by filling with 0\n",
    "df[numeric_columns].fillna(0, inplace=True)\n",
    "\n",
    "# Scale the numeric columns using MinMaxScaler\n",
    "scaler = MinMaxScaler()  #  the following function changed here form on comarison to the standard function\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Output cleaned DataFrame for verification\n",
    "print(df[numeric_columns])\n",
    "\n",
    "# Save the normalized data back into a CSV file if needed\n",
    "df.to_csv('normalized_menu_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69e6259-e988-464a-8a14-097c03df3052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bef892-0d69-4130-a43e-c31853666be6",
   "metadata": {},
   "source": [
    "Detailed Plan\n",
    "    One-Hot Encode Categorical Columns:\n",
    "        Apply one-hot encoding to Meal_Type and Food_Item columns using pandas.get_dummies().\n",
    "    Concatenate the Encoded Columns with the Normalized Data:\n",
    "        Combine the newly created one-hot encoded columns with the normalized numeric columns.\n",
    "            Save or Use the Encoded Data:\n",
    "            You can save the encoded DataFrame or use it directly for training.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8899be-526c-482c-9fa9-9fd9c81cf557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Calories Carbs (g) Protein (g)  Fat (g) Fiber (g)  \\\n",
      "0       58 kcal      12 g       1.6 g    0.4 g     0.5 g   \n",
      "1  150-200 kcal      20 g         2 g  10-12 g     2-3 g   \n",
      "2  150-200 kcal   25-30 g       5-7 g    3-4 g     5-7 g   \n",
      "3       60 kcal     4.9 g       1.3 g    4.4 g     1.7 g   \n",
      "4      212 kcal      29 g       5.5 g    8.2 g       1 g   \n",
      "\n",
      "            Other Key Nutrients  Unnamed: 8  Category_Breakfast  \\\n",
      "0  Sodium: 75 mg, Iron: 3.3% DV         NaN                True   \n",
      "1                             -         NaN               False   \n",
      "2  Vitamin A: 23%, Iron: 27% DV         NaN               False   \n",
      "3             Vitamin C: 81% DV         NaN               False   \n",
      "4                 Calcium, Iron         NaN               False   \n",
      "\n",
      "   Category_Dinner  Category_Lunch  ...  Dish_Milk (1 cup)  \\\n",
      "0            False           False  ...              False   \n",
      "1            False           False  ...              False   \n",
      "2            False           False  ...              False   \n",
      "3            False           False  ...              False   \n",
      "4            False           False  ...              False   \n",
      "\n",
      "   Dish_Paneer Curry (1 cup)  Dish_Pongal (1 serving)  Dish_Raita (1/2 cup)  \\\n",
      "0                      False                    False                 False   \n",
      "1                      False                    False                 False   \n",
      "2                      False                    False                 False   \n",
      "3                      False                    False                 False   \n",
      "4                      False                     True                 False   \n",
      "\n",
      "   Dish_Rice (1 cup)  Dish_Sambar (1 cup)  Dish_Scrambled Eggs (2)  \\\n",
      "0              False                False                    False   \n",
      "1              False                False                    False   \n",
      "2              False                 True                    False   \n",
      "3              False                False                    False   \n",
      "4              False                False                    False   \n",
      "\n",
      "   Dish_Tea (with milk)  Dish_Vada (1 piece)  Dish_Veg Pulao (1 cup)  \n",
      "0                 False                False                   False  \n",
      "1                 False                 True                   False  \n",
      "2                 False                False                   False  \n",
      "3                 False                False                   False  \n",
      "4                 False                False                   False  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# One-hot encode the output labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('/Users/Temporary/Downloads/menu_data.csv')\n",
    "\n",
    "# Step 2: One-Hot Encode categorical columns (Meal_Type and Food_Item)\n",
    "df_encoded = pd.get_dummies(df, columns=['Category', 'Dish'])\n",
    "\n",
    "# Define numeric columns\n",
    "numeric_columns = ['Carbs (g)', 'Protein (g)', 'Fat (g)', 'Fiber (g)']\n",
    "\n",
    "# Define input features (X) and output labels (y)\n",
    "X = df[numeric_columns]  # Nutritional values\n",
    "y = df['Category']  # Assuming 'Category' holds meal types\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Step 3: (Optional) Print the first few rows to verify the encoding and normalization\n",
    "print(df_encoded.head())\n",
    "\n",
    "# Step 4: Save the fully preprocessed data to a new CSV file\n",
    "df_encoded.to_csv('preprocessed_menu_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b4cae-0b1a-446a-9662-67b69fa5378f",
   "metadata": {},
   "source": [
    "Imported TensorFlow/Keras: To build and train the neural network.\n",
    "Defined the Neural Network Architecture:\n",
    "Input Layer: 7 input neurons (nutrition + user inputs).\n",
    "Hidden Layers: 128, 64, and 32 neurons with ReLU.\n",
    "Output Layer: 3 neurons for meal category classification (breakfast, lunch, dinner) using softmax.\n",
    "Compiled the Model: Using Adam optimizer and categorical cross-entropy loss for classification.\n",
    "Trained the Model: On preprocessed data with validation set.\n",
    "Evaluated the Model: Checked accuracy on the validation set.\n",
    "Made Predictions: On new user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d756572b-db50-494a-8a4c-7b4985537aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.12.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from tensorflow) (74.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.66.2-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow)\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow)\n",
      "  Downloading rich-13.9.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow)\n",
      "  Downloading optree-0.13.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.17.0-cp312-cp312-macosx_12_0_arm64.whl (236.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.66.2-cp312-cp312-macosx_10_9_universal2.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp312-cp312-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl (405 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.0-cp312-cp312-macosx_11_0_arm64.whl (305 kB)\n",
      "Downloading rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, typing-extensions, termcolor, tensorboard-data-server, protobuf, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, optree, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.1\n",
      "    Uninstalling numpy-2.1.1:\n",
      "      Successfully uninstalled numpy-2.1.1\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.66.2 h5py-3.12.1 keras-3.6.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 opt-einsum-3.4.0 optree-0.13.0 protobuf-4.25.5 rich-13.9.2 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 termcolor-2.4.0 typing-extensions-4.12.2 werkzeug-3.0.4 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1469ce4e-60c3-47eb-9275-6b1f60e5f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/wl7ddbqs0_d8c_g077wn429h0000gp/T/ipykernel_37086/2075688113.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[numeric_columns].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler  # Change here\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/Temporary/Downloads/menu_data.csv')\n",
    "\n",
    "# Define numeric columns\n",
    "numeric_columns = ['Carbs (g)', 'Protein (g)', 'Fat (g)', 'Fiber (g)']\n",
    "\n",
    "# Function to clean and convert columns\n",
    "def clean_numeric_column(col):\n",
    "    return pd.to_numeric(col.str.extract(r'(\\d+\\.?\\d*)')[0], errors='coerce')\n",
    "\n",
    "# Clean each numeric column\n",
    "for col in numeric_columns:\n",
    "    df[col] = clean_numeric_column(df[col])\n",
    "\n",
    "# Handle NaN values by filling with 0\n",
    "df[numeric_columns].fillna(0, inplace=True)\n",
    "# Assuming you have a DataFrame or numpy array for X (input features) and y (output labels)\n",
    "# X will have the features: Carbs, Proteins, Fats, Fiber, Calories, Calorie Goal, Diet Goal\n",
    "# y will have the target labels (one-hot encoded meal categories)\n",
    "\n",
    "# Example:\n",
    "# X = your_feature_matrix  # Features (carbs, proteins, fats, fiber, etc.)\n",
    "# y = your_label_matrix  # One-hot encoded meal categories\n",
    "# Define input features (X) and output labels (y)\n",
    "X = df[numeric_columns]  # Nutritional values\n",
    "y = df['Category']  # Assuming 'Category' holds meal types\n",
    "\n",
    "# Split the data into 80% training and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a78029f-1bab-4f66-8db5-3a114e35f71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), \n\u001b[1;32m     16\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     17\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Assuming X_train is the input features (carbs, proteins, fats, fiber, calories, calorie goal, diet goal)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# And y_train is the output labels (e.g., one-hot encoded meal categories)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Evaluate the model on validation data\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "# INPUT LAYER\n",
    "model.add(Dense(128,input_dim = 7 ,activation='relu'))\n",
    "# HIDDEN LAYER\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "# OUTPUT LAYER -- using the softmax function for  the prediction of the meals \n",
    "model.add(Dense(3,activation='softmax'))\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "# Assuming X_train is the input features (carbs, proteins, fats, fiber, calories, calorie goal, diet goal)\n",
    "# And y_train is the output labels (e.g., one-hot encoded meal categories)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
    "# Evaluate the model on validation data\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "# Train the model on the training data, with validation data\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6613a0e-da4c-4cff-871a-b0501e274544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c0450-e27f-470f-b4b1-de2f3324a591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a258a-4055-4f5b-ac2a-6a02ac52bbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dffcc40-fc28-471b-91ff-ab1efd27ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Category                Dish      Calories Carbs (g) Protein (g)  Fat (g)  \\\n",
      "0  Breakfast      Idly (1 piece)       58 kcal      12 g       1.6 g    0.4 g   \n",
      "1        NaN      Vada (1 piece)  150-200 kcal      20 g         2 g  10-12 g   \n",
      "2        NaN      Sambar (1 cup)  150-200 kcal   25-30 g       5-7 g    3-4 g   \n",
      "3        NaN    Chutney (2 tbsp)       60 kcal     4.9 g       1.3 g    4.4 g   \n",
      "4        NaN  Pongal (1 serving)      212 kcal      29 g       5.5 g    8.2 g   \n",
      "\n",
      "  Fiber (g)           Other Key Nutrients  Unnamed: 8  \n",
      "0     0.5 g  Sodium: 75 mg, Iron: 3.3% DV         NaN  \n",
      "1     2-3 g                             -         NaN  \n",
      "2     5-7 g  Vitamin A: 23%, Iron: 27% DV         NaN  \n",
      "3     1.7 g             Vitamin C: 81% DV         NaN  \n",
      "4       1 g                 Calcium, Iron         NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Users/Temporary/Downloads/menu_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7887d33a-7914-4b43-b9fb-373a37b88730",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf0ffa4-0e7f-4e12-b266-dad2f7f497fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Carbs (g)  Protein (g)  Fat (g)  Fiber (g)  Calories\n",
      "0        12.0          1.6      0.4        0.5      58.0\n",
      "1        20.0          2.0     11.0        2.5     175.0\n",
      "2        27.5          6.0      3.5        6.0     175.0\n",
      "3         4.9          1.3      4.4        1.7      60.0\n",
      "4        29.0          5.5      8.2        1.0     212.0\n",
      "5        27.5          5.0      1.5        2.0     140.0\n",
      "6         2.0         12.0     14.0        0.0     180.0\n",
      "7         0.0          0.0      4.0        0.0      35.0\n",
      "8        13.0          0.0      0.0        0.0      50.0\n",
      "9         0.0          0.0      0.0        0.0       2.0\n",
      "10        6.0          1.5      2.0        0.0      55.0\n",
      "11       42.5         17.5     11.0        2.5     305.0\n",
      "12        5.5          3.5      2.5        0.0      70.0\n",
      "13       12.5         16.5     27.5        1.5     325.0\n",
      "14       32.5          5.5      7.0        3.5     215.0\n",
      "15        6.0          1.0      0.0        2.0      25.0\n",
      "16       33.0          3.0      0.0        0.6     150.0\n",
      "17       22.0          7.0      4.5        5.5     140.0\n",
      "18       22.5          2.0      9.0        3.0     165.0\n",
      "19        3.5          3.5      2.5        0.0      60.0\n",
      "20       15.0          2.5      1.5        2.0      85.0\n",
      "21       12.0          8.0      6.0        0.0     125.0\n",
      "Cleaned data saved to 'cleaned_menu_data.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/wl7ddbqs0_d8c_g077wn429h0000gp/T/ipykernel_46205/310401456.py:28: ChainedAssignmentError: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "When using the Copy-on-Write mode, such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[numeric_columns].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/Temporary/Downloads/menu_data.csv')  # Update with your actual path\n",
    "\n",
    "# Function to clean and convert columns\n",
    "def clean_nutritional_values(col):\n",
    "    # Extract numeric values and handle ranges and units\n",
    "    def parse_value(value):\n",
    "        if isinstance(value, str):\n",
    "            # Check for ranges\n",
    "            if '-' in value:\n",
    "                lower, upper = value.split('-')\n",
    "                return (float(lower.strip().replace(' g', '').replace(' kcal', '')) + \n",
    "                        float(upper.strip().replace(' g', '').replace(' kcal', ''))) / 2  # Take the average\n",
    "            else:\n",
    "                return float(value.replace(' g', '').replace(' kcal', '').strip())\n",
    "        return value\n",
    "    \n",
    "    return col.apply(parse_value)\n",
    "\n",
    "# Clean numeric columns\n",
    "numeric_columns = ['Carbs (g)', 'Protein (g)', 'Fat (g)', 'Fiber (g)', 'Calories']\n",
    "for col in numeric_columns:\n",
    "    df[col] = clean_nutritional_values(df[col])\n",
    "\n",
    "# Fill NaN values with 0\n",
    "df[numeric_columns].fillna(0, inplace=True)\n",
    "\n",
    "# Output cleaned DataFrame for verification\n",
    "print(df[numeric_columns])\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv('/Users/Temporary/Downloads/cleaned_menu_data.csv', index=False)\n",
    "\n",
    "print(\"Cleaned data saved to 'cleaned_menu_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fea72bf-c779-4b75-bb21-54ffd1b7a779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504ms/step - accuracy: 0.1176 - loss: 1.3332 - val_accuracy: 0.4000 - val_loss: 1.3102\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3529 - loss: 1.3068 - val_accuracy: 0.6000 - val_loss: 1.2850\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7059 - loss: 1.2811 - val_accuracy: 0.8000 - val_loss: 1.2600\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9412 - loss: 1.2554 - val_accuracy: 0.8000 - val_loss: 1.2344\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 1.2294 - val_accuracy: 0.8000 - val_loss: 1.2093\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 1.2023 - val_accuracy: 0.8000 - val_loss: 1.1837\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8824 - loss: 1.1745 - val_accuracy: 0.8000 - val_loss: 1.1576\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 1.1461 - val_accuracy: 0.8000 - val_loss: 1.1310\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 1.1174 - val_accuracy: 0.8000 - val_loss: 1.1041\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 1.0881 - val_accuracy: 0.8000 - val_loss: 1.0776\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 1.0581 - val_accuracy: 0.8000 - val_loss: 1.0508\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 1.0273 - val_accuracy: 0.8000 - val_loss: 1.0224\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.9956 - val_accuracy: 0.8000 - val_loss: 0.9944\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.9637 - val_accuracy: 0.8000 - val_loss: 0.9668\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8824 - loss: 0.9321 - val_accuracy: 0.8000 - val_loss: 0.9397\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8824 - loss: 0.9012 - val_accuracy: 0.8000 - val_loss: 0.9137\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.8710 - val_accuracy: 0.8000 - val_loss: 0.8891\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.8418 - val_accuracy: 0.8000 - val_loss: 0.8660\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.8138 - val_accuracy: 0.8000 - val_loss: 0.8444\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.7875 - val_accuracy: 0.8000 - val_loss: 0.8246\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.7630 - val_accuracy: 0.8000 - val_loss: 0.8067\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8824 - loss: 0.7404 - val_accuracy: 0.8000 - val_loss: 0.7910\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.7199 - val_accuracy: 0.8000 - val_loss: 0.7775\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.7015 - val_accuracy: 0.8000 - val_loss: 0.7665\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.6851 - val_accuracy: 0.8000 - val_loss: 0.7576\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.6705 - val_accuracy: 0.8000 - val_loss: 0.7511\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.6576 - val_accuracy: 0.8000 - val_loss: 0.7470\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8824 - loss: 0.6462 - val_accuracy: 0.8000 - val_loss: 0.7450\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8824 - loss: 0.6361 - val_accuracy: 0.8000 - val_loss: 0.7448\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.6269 - val_accuracy: 0.8000 - val_loss: 0.7463\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.6184 - val_accuracy: 0.8000 - val_loss: 0.7492\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.6103 - val_accuracy: 0.8000 - val_loss: 0.7534\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.6024 - val_accuracy: 0.8000 - val_loss: 0.7586\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.5942 - val_accuracy: 0.8000 - val_loss: 0.7646\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.5858 - val_accuracy: 0.8000 - val_loss: 0.7710\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.5770 - val_accuracy: 0.8000 - val_loss: 0.7776\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8824 - loss: 0.5678 - val_accuracy: 0.8000 - val_loss: 0.7843\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8824 - loss: 0.5582 - val_accuracy: 0.8000 - val_loss: 0.7911\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.5482 - val_accuracy: 0.8000 - val_loss: 0.7979\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8824 - loss: 0.5379 - val_accuracy: 0.8000 - val_loss: 0.8048\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8824 - loss: 0.5272 - val_accuracy: 0.8000 - val_loss: 0.8119\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.5164 - val_accuracy: 0.8000 - val_loss: 0.8187\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8824 - loss: 0.5054 - val_accuracy: 0.8000 - val_loss: 0.8252\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.4945 - val_accuracy: 0.8000 - val_loss: 0.8316\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8824 - loss: 0.4834 - val_accuracy: 0.8000 - val_loss: 0.8379\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8824 - loss: 0.4724 - val_accuracy: 0.8000 - val_loss: 0.8438\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.4618 - val_accuracy: 0.8000 - val_loss: 0.8493\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.4512 - val_accuracy: 0.8000 - val_loss: 0.8548\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.4408 - val_accuracy: 0.8000 - val_loss: 0.8599\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8824 - loss: 0.4305 - val_accuracy: 0.8000 - val_loss: 0.8648\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8000 - loss: 0.8648\n",
      "Validation Loss: 0.8648052215576172, Validation Accuracy: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed CSV file\n",
    "df = pd.read_csv('/Users/Temporary/Downloads/cleaned_menu_data.csv')\n",
    "\n",
    "# Step 1: One-Hot Encode categorical columns (Category and Dish)\n",
    "df_encoded = pd.get_dummies(df, columns=['Category', 'Dish'])\n",
    "\n",
    "# Step 2: Define input features (X) and output labels (y)\n",
    "numeric_columns = ['Carbs (g)', 'Protein (g)', 'Fat (g)', 'Fiber (g)']  # Ensure you select only numeric columns\n",
    "X = df[numeric_columns]  # Nutritional values\n",
    "y = df['Category']  # Assuming 'Category' holds meal types\n",
    "\n",
    "# Step 3: One-Hot Encode 'y' (meal categories like 'Breakfast', 'Lunch', etc.)\n",
    "encoder = OneHotEncoder(sparse_output = False)\n",
    "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Step 4: Normalize the input features (Optional but recommended)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 5: Split the data into 80% training and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Define the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Input Layer (4 input neurons - Carbs, Proteins, Fats, Fiber)\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "\n",
    "# Add Hidden Layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the Output Layer (Softmax for classification)\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))  # Output layer with the number of meal categories\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa36dbe7-31fe-4ebf-87eb-f0133f6b55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d81ae06-0b37-40e7-8790-55e55c3cd79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a506aa49-f405-4155-970b-124c5ec65f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use sparse=False to get a dense array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1375cc2-b759-48d5-9487-19d2259734d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Use sparse_output=False to get a dense array\n",
    "encoder = OneHotEncoder(sparse_output=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cb69959-c2e6-406a-8358-5f68ca39e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('meal_recommender_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe28e763-5100-4d08-95dc-c502ed6c5d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Predicted Category: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 10 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_category\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBreakfast\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDinner\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m predicted_meal \u001b[38;5;241m=\u001b[39m \u001b[43mcategories\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_category\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggested Meal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_meal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('meal_recommender_model.keras')\n",
    "# Assuming the user's input is in the form of nutritional values (Carbs, Proteins, Fats, Fiber)\n",
    "user_input = np.array([[60, 20, 10, 5]])  # Example user input\n",
    "\n",
    "# Normalize the user input (using the same scaler used during training)\n",
    "user_input_scaled = scaler.transform(user_input)\n",
    "# Predict meal category based on user's input\n",
    "predictions = model.predict(user_input_scaled)\n",
    "\n",
    "# Get the predicted meal category\n",
    "predicted_category = np.argmax(predictions)\n",
    "print(f\"Predicted Category: {predicted_category}\")\n",
    "\n",
    "categories = ['Breakfast', 'Lunch', 'Dinner']\n",
    "predicted_meal = categories[predicted_category]\n",
    "print(f\"Suggested Meal: {predicted_meal}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4704b524-bb19-4b92-9588-54e2b4747d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Number of Categories: 4\n",
      "Error: Predicted category index is out of range.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 10 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the model\n",
    "model = load_model('meal_recommender_model.keras')\n",
    "\n",
    "# Assuming the user's input is in the form of nutritional values (Carbs, Proteins, Fats, Fiber)\n",
    "user_input = np.array([[60, 20, 10, 5]])  # Example user input\n",
    "\n",
    "# Normalize the user input (using the same scaler used during training)\n",
    "user_input_scaled = scaler.transform(user_input)\n",
    "\n",
    "# Predict meal category based on user's input\n",
    "predictions = model.predict(user_input_scaled)\n",
    "\n",
    "# Get the predicted meal category\n",
    "predicted_category = np.argmax(predictions)\n",
    "\n",
    "# Check the number of categories in your model's output\n",
    "num_categories = predictions.shape[1]\n",
    "print(f\"Number of Categories: {num_categories}\")\n",
    "\n",
    "# Define categories based on your training data\n",
    "categories = ['Breakfast', 'Lunch', 'Dinner']\n",
    "\n",
    "# Ensure predicted_category is within bounds\n",
    "if predicted_category < len(categories):\n",
    "    predicted_meal = categories[predicted_category]\n",
    "    print(f\"Suggested Meal: {predicted_meal}\")\n",
    "else:\n",
    "    print(\"Error: Predicted category index is out of range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "829b706b-2e2d-4669-9fd4-6e571a85c83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 10 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     73\u001b[0m total_calories \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m  \u001b[38;5;66;03m# Example input\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m suggested_meals \u001b[38;5;241m=\u001b[39m \u001b[43msuggest_meals_for_day\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_calories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggested Breakfast: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_meals[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggested Lunch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_meals[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 65\u001b[0m, in \u001b[0;36msuggest_meals_for_day\u001b[0;34m(model, scaler, total_calories)\u001b[0m\n\u001b[1;32m     62\u001b[0m predicted_dinner \u001b[38;5;241m=\u001b[39m predict_meal(model, dinner_input)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Step 4: Map predictions to meal categories\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m suggested_breakfast \u001b[38;5;241m=\u001b[39m \u001b[43mmap_category_to_meal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_breakfast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m suggested_lunch \u001b[38;5;241m=\u001b[39m map_category_to_meal(predicted_lunch)\n\u001b[1;32m     67\u001b[0m suggested_dinner \u001b[38;5;241m=\u001b[39m map_category_to_meal(predicted_dinner)\n",
      "Cell \u001b[0;32mIn[24], line 47\u001b[0m, in \u001b[0;36mmap_category_to_meal\u001b[0;34m(predicted_category)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_category_to_meal\u001b[39m(predicted_category):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcategories\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_category\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def distribute_calorie_goal(total_calories):\n",
    "    # Define the calorie distribution (adjust the percentages as per your app's logic)\n",
    "    breakfast_calories = total_calories * 0.3\n",
    "    lunch_calories = total_calories * 0.4\n",
    "    dinner_calories = total_calories * 0.3\n",
    "    return breakfast_calories, lunch_calories, dinner_calories\n",
    "def preprocess_input(calories, carbs_percent, proteins_percent, fats_percent, fiber_percent, scaler):\n",
    "    # Convert calories into approximate nutrient breakdown\n",
    "    carbs = calories * carbs_percent / 100\n",
    "    proteins = calories * proteins_percent / 100\n",
    "    fats = calories * fats_percent / 100\n",
    "    fiber = calories * fiber_percent / 100\n",
    "\n",
    "    # Create an input array\n",
    "    input_array = np.array([[carbs, proteins, fats, fiber]])\n",
    "\n",
    "    # Normalize the input using the scaler\n",
    "    input_scaled = scaler.transform(input_array)\n",
    "    return input_scaled\n",
    "def predict_meal(model, input_scaled):\n",
    "    # Predict the meal category\n",
    "    predictions = model.predict(input_scaled)\n",
    "    predicted_category = np.argmax(predictions)\n",
    "    return predicted_category\n",
    "\n",
    "categories = ['Breakfast', 'Lunch', 'Dinner']\n",
    "\n",
    "def map_category_to_meal(predicted_category):\n",
    "    return categories[predicted_category]\n",
    "# Assuming the model and scaler are already loaded\n",
    "def suggest_meals_for_day(model, scaler, total_calories):\n",
    "    # Step 1: Distribute the calorie goal\n",
    "    breakfast_calories, lunch_calories, dinner_calories = distribute_calorie_goal(total_calories)\n",
    "\n",
    "    # Step 2: Preprocess the input for each meal\n",
    "    # Nutrient percentage breakdown for each meal type (these values can be adjusted)\n",
    "    breakfast_input = preprocess_input(breakfast_calories, carbs_percent=50, proteins_percent=20, fats_percent=20, fiber_percent=10, scaler=scaler)\n",
    "    lunch_input = preprocess_input(lunch_calories, carbs_percent=50, proteins_percent=25, fats_percent=20, fiber_percent=5, scaler=scaler)\n",
    "    dinner_input = preprocess_input(dinner_calories, carbs_percent=45, proteins_percent=30, fats_percent=20, fiber_percent=5, scaler=scaler)\n",
    "\n",
    "    # Step 3: Make predictions for each meal\n",
    "    predicted_breakfast = predict_meal(model, breakfast_input)\n",
    "    predicted_lunch = predict_meal(model, lunch_input)\n",
    "    predicted_dinner = predict_meal(model, dinner_input)\n",
    "\n",
    "    # Step 4: Map predictions to meal categories\n",
    "    suggested_breakfast = map_category_to_meal(predicted_breakfast)\n",
    "    suggested_lunch = map_category_to_meal(predicted_lunch)\n",
    "    suggested_dinner = map_category_to_meal(predicted_dinner)\n",
    "\n",
    "    # Return suggested meals for breakfast, lunch, and dinner\n",
    "    return suggested_breakfast, suggested_lunch, suggested_dinner\n",
    "\n",
    "# Example usage:\n",
    "total_calories = 2000  # Example input\n",
    "suggested_meals = suggest_meals_for_day(model, scaler, total_calories)\n",
    "\n",
    "print(f\"Suggested Breakfast: {suggested_meals[0]}\")\n",
    "print(f\"Suggested Lunch: {suggested_meals[1]}\")\n",
    "print(f\"Suggested Dinner: {suggested_meals[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b8087-8e59-4f35-907d-42997a6781ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
